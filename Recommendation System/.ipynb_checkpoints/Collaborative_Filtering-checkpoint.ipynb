{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:10:12.493907Z",
     "start_time": "2019-03-15T20:10:12.490309Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:11:17.117457Z",
     "start_time": "2019-03-15T20:11:17.111449Z"
    }
   },
   "outputs": [],
   "source": [
    "# here is a handy function from fast.ai\n",
    "def proc_col(col):\n",
    "    \"\"\"Encodes a pandas column with continous ids. \n",
    "    \"\"\"\n",
    "    uniq = col.unique()\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return name2idx, np.array([name2idx[x] for x in col]), len(uniq)\n",
    "    \n",
    "\n",
    "def encode_data(df):\n",
    "    \"\"\"Encodes rating data with continous user and movie ids using \n",
    "    the helpful fast.ai function from above.\n",
    "    \n",
    "    Arguments:\n",
    "      train_csv: a csv file with columns user_id,movie_id,rating \n",
    "    \n",
    "    Returns:\n",
    "      df: a dataframe with the encode data\n",
    "      num_users\n",
    "      num_movies\n",
    "      \n",
    "    \"\"\"\n",
    "    # call this function for userId and movieId column of the dataframe\n",
    "    user2id, userArray, num_users= proc_col(df.userId)\n",
    "    movie2id, movieArray, num_movies= proc_col(df.movieId)\n",
    "    df['userId']= userArray\n",
    "    df['movieId']= movieArray\n",
    "    return df, num_users, num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:11:35.978556Z",
     "start_time": "2019-03-15T20:11:35.974194Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedings(n, K):\n",
    "    \"\"\" Creating a numpy random matrix of shape n, K initialized with uniform values in (0, 6/K)\n",
    "    Arguments:\n",
    "    \n",
    "    Inputs:\n",
    "    n: number of items/users\n",
    "    K: number of factors in the embeding \n",
    "    \n",
    "    Returns:\n",
    "    emb: numpy array of shape (n, num_factors)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    emb = 6*np.random.random((n, K)) / K\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:14:38.413876Z",
     "start_time": "2019-03-15T20:14:38.324226Z"
    }
   },
   "outputs": [],
   "source": [
    "## Encoding Y as a sparse matrix\n",
    "from scipy import sparse\n",
    "def df2matrix(df, nrows, ncols, column_name=\"rating\"):\n",
    "    \"\"\" Returns a sparse matrix constructed from a dataframe\n",
    "    This code assumes the df has columns: MovieID,UserID,Rating\n",
    "    \"\"\"\n",
    "    values = df[column_name].values\n",
    "    ind_movie = df['movieId'].values\n",
    "    ind_user = df['userId'].values\n",
    "    return sparse.csc_matrix((values,(ind_user, ind_movie)),shape=(nrows, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:15:20.869608Z",
     "start_time": "2019-03-15T20:15:20.865288Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(df, emb_user, emb_movie):\n",
    "    \"\"\" This function computes df[\"prediction\"] without doing (U*V^T)\n",
    "    \"\"\"\n",
    " \n",
    "    df['prediction']=np.sum(np.multiply(emb_user[df['userId']],emb_movie[df['movieId']]),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:15:58.224040Z",
     "start_time": "2019-03-15T20:15:58.219718Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost(df, emb_user, emb_movie):\n",
    "    \"\"\" Computes mean square error\n",
    "    Prediction for user i and movie j is emb_user[i]*emb_movie[j]\n",
    "    \n",
    "    Arguments:\n",
    "      df: dataframe with all data or a subset of the data\n",
    "      emb_user: embedings for users\n",
    "      emb_movie: embedings for movies\n",
    "      \n",
    "    Returns:\n",
    "      error(float): this is the MSE\n",
    "    \"\"\"\n",
    "    predict(df,emb_user, emb_movie)\n",
    "    error = sum((df['prediction']-df['rating'])**2)/len(df)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:16:28.024437Z",
     "start_time": "2019-03-15T20:16:28.018239Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(df, emb_user, emb_movie):\n",
    "    \"\"\" Computes the gradient.\n",
    "    Arguments:\n",
    "      df: dataframe with all data or a subset of the data\n",
    "      Y: sparse representation of df\n",
    "      emb_user: embedings for users\n",
    "      emb_movie: embedings for movies\n",
    "      \n",
    "    Returns:\n",
    "      d_emb_user\n",
    "      d_emb_movie\n",
    "    \"\"\"\n",
    "    df = predict(df,emb_movie=emb_movie, emb_user=emb_user)\n",
    "    Y = df2matrix(df,emb_user.shape[0], emb_movie.shape[0])\n",
    "    Y_hat = df2matrix(df,emb_user.shape[0], emb_movie.shape[0], column_name='prediction')\n",
    "    R = Y > 0\n",
    "    e = Y - Y_hat\n",
    "    grad = e.multiply(R) \n",
    "    d_emb_user= (-2/len(df))*(grad.dot(emb_movie))\n",
    "    d_emb_movie= (-2/len(df))* (grad.transpose().dot(emb_user))\n",
    "    return d_emb_user, d_emb_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:16:50.370400Z",
     "start_time": "2019-03-15T20:16:50.363358Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(df, emb_user, emb_movie, iterations=100, learning_rate=0.01, df_val=None):\n",
    "    \"\"\" Computes gradient descent with momentum (0.9) for a number of iterations.\n",
    "    Prints training cost and validation cost (if df_val is not None) every 50 iterations.\n",
    "    \n",
    "    Returns:\n",
    "    emb_user: the trained user embedding\n",
    "    emb_movie: the trained movie embedding\n",
    "    \"\"\"\n",
    "    Y = df2matrix(df, emb_user.shape[0], emb_movie.shape[0])\n",
    "    beta=0.9\n",
    "    for i in range(iterations):\n",
    "        d_emb_user, d_emb_movie = gradient(df,emb_movie=emb_movie, emb_user=emb_user)\n",
    "        momentum_user= d_emb_user\n",
    "        momentum_movie= d_emb_movie\n",
    "        momentum_user=beta*momentum_user + (1-beta)*d_emb_user\n",
    "        momentum_movie=beta*momentum_movie + (1-beta)*d_emb_movie\n",
    "        \n",
    "        emb_user= emb_user - learning_rate*momentum_user\n",
    "        emb_movie= emb_movie - learning_rate*momentum_movie\n",
    "        \n",
    "        if (i+1)%50==0:\n",
    "            print('training loss: '+ str(cost(df,emb_user,emb_movie)))\n",
    "            if df_val is not None: \n",
    "                print('validation cost: '+ str(cost(df_val,emb_user,emb_movie)))\n",
    "    return emb_user, emb_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data\n",
    "Now we should write a function that given new data is able to predict ratings. First we write a function that encodes new data. If a new user or item is present that row should be remove. Collaborative Filtering is not good at handling new users or new items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:17:32.661185Z",
     "start_time": "2019-03-15T20:17:32.657282Z"
    }
   },
   "outputs": [],
   "source": [
    "def proc_col_new(col, mapping):\n",
    "    new_mapping=list()\n",
    "    for o in col:\n",
    "        if o in mapping.keys(): new_mapping.append(mapping[o])\n",
    "        else: new_mapping.append(None)\n",
    "    return new_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:18:04.288233Z",
     "start_time": "2019-03-15T20:18:04.283365Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_new_data(df_val, df_train):\n",
    "    \"\"\" Encodes df_val with the same encoding as df_train.\n",
    "    Returns:\n",
    "    df_val: dataframe with the same encoding as df_train\n",
    "    \"\"\"\n",
    "    user2id, userArray, num_users= proc_col(df_train.userId)\n",
    "    movie2id, movieArray, num_movies= proc_col(df_train.movieId)\n",
    "    df_train['userId']= userArray\n",
    "    df_train['movieId']= movieArray\n",
    "\n",
    "    df_val['userId']=proc_col_new(df_val['userId'],user2id)\n",
    "    df_val['movieId']=proc_col_new(df_val['movieId'],movie2id)\n",
    "\n",
    "    df_val=df_val.dropna()\n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o movielens-ml-latest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:23:39.307088Z",
     "start_time": "2019-03-15T20:23:39.038525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20386 19591\n"
     ]
    }
   ],
   "source": [
    "path = \"../ml-latest-small/\"\n",
    "data = pd.read_csv(path + \"ratings.csv\")\n",
    "# sorting by timestamp take as validation data the most recent data doesn't work so let's just take 20%\n",
    "# at random\n",
    "np.random.seed(3)\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk].copy()\n",
    "val = data[~msk].copy()\n",
    "df_train, num_users, num_movies = encode_data(train.copy())\n",
    "df_val = encode_new_data(val.copy(), train.copy())\n",
    "df_val.movieId=df_val.movieId.astype('int64')\n",
    "print(len(val), len(df_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:26:06.490317Z",
     "start_time": "2019-03-15T20:23:41.607112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 9.396604197054799\n",
      "validation cost: 9.522214163589737\n",
      "training loss: 6.571786860324157\n",
      "validation cost: 6.7001849104095115\n",
      "training loss: 4.731420690900383\n",
      "validation cost: 4.849609948607673\n",
      "training loss: 3.707740929307275\n",
      "validation cost: 3.813395864951322\n",
      "training loss: 3.065219657270483\n",
      "validation cost: 3.1587983632545846\n",
      "training loss: 2.621268689059941\n",
      "validation cost: 2.7060732677805333\n",
      "training loss: 2.2980596955361214\n",
      "validation cost: 2.377359634836783\n",
      "training loss: 2.053985734006945\n",
      "validation cost: 2.1300940459911653\n",
      "training loss: 1.8641522365835246\n",
      "validation cost: 1.9385780990036197\n",
      "training loss: 1.7128195272542859\n",
      "validation cost: 1.786545355717238\n",
      "training loss: 1.589645154487459\n",
      "validation cost: 1.6633225619448928\n",
      "training loss: 1.4876078081218607\n",
      "validation cost: 1.5616785240222497\n",
      "training loss: 1.4018012176887606\n",
      "validation cost: 1.4765711191093203\n",
      "training loss: 1.3287079625615101\n",
      "validation cost: 1.404393377525737\n",
      "training loss: 1.265748051772432\n",
      "validation cost: 1.3425053773318019\n",
      "training loss: 1.2109899453785757\n",
      "validation cost: 1.2889349307111355\n",
      "training loss: 1.1629606215733141\n",
      "validation cost: 1.2421810974379597\n",
      "training loss: 1.120517795458849\n",
      "validation cost: 1.2010821802005671\n",
      "training loss: 1.082762145779492\n",
      "validation cost: 1.164725199105048\n",
      "training loss: 1.0489758735795578\n",
      "validation cost: 1.1323826410555868\n",
      "training loss: 1.018578933397204\n",
      "validation cost: 1.1034674915887488\n",
      "training loss: 0.9910973365172413\n",
      "validation cost: 1.0775007358038997\n",
      "training loss: 0.9661398372593459\n",
      "validation cost: 1.054087502318687\n",
      "training loss: 0.9433805322493921\n",
      "validation cost: 1.0328992916289823\n",
      "training loss: 0.9225456934378076\n",
      "validation cost: 1.0136605522152595\n",
      "training loss: 0.9034036764851777\n",
      "validation cost: 0.9961384086305832\n",
      "training loss: 0.8857570940199843\n",
      "validation cost: 0.9801347065756969\n",
      "training loss: 0.8694366787740447\n",
      "validation cost: 0.9654797837629252\n",
      "training loss: 0.8542964231552981\n",
      "validation cost: 0.9520275422722687\n",
      "training loss: 0.8402096941023363\n",
      "validation cost: 0.9396515138592791\n",
      "training loss: 0.8270661011301177\n",
      "validation cost: 0.9282416910092329\n",
      "training loss: 0.8147689518545483\n",
      "validation cost: 0.9177019544202644\n",
      "training loss: 0.8032331699859541\n",
      "validation cost: 0.9079479693178836\n",
      "training loss: 0.7923835805109958\n",
      "validation cost: 0.8989054534376589\n",
      "training loss: 0.7821534887429387\n",
      "validation cost: 0.8905087419689907\n",
      "training loss: 0.7724834963128988\n",
      "validation cost: 0.8826995915043655\n",
      "training loss: 0.7633205095339801\n",
      "validation cost: 0.8754261776601914\n",
      "training loss: 0.7546169049736268\n",
      "validation cost: 0.868642250635452\n",
      "training loss: 0.7463298242881494\n",
      "validation cost: 0.8623064203404974\n",
      "training loss: 0.7384205759576179\n",
      "validation cost: 0.8563815484250422\n"
     ]
    }
   ],
   "source": [
    "K = 50\n",
    "emb_user = create_embedings(num_users, K)\n",
    "emb_movie = create_embedings(num_movies, K)\n",
    "emb_user, emb_movie = gradient_descent(df_train, emb_user, emb_movie, iterations=2000, learning_rate=1, df_val=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T20:26:06.701682Z",
     "start_time": "2019-03-15T20:26:06.614967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7384205759576179 0.8563815484250422\n"
     ]
    }
   ],
   "source": [
    "train_mse = cost(df_train, emb_user, emb_movie)\n",
    "val_mse = cost(df_val, emb_user, emb_movie)\n",
    "print(train_mse, val_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the embeddings are trained. We can use them for calculating predicted ratings, for computing item-item similarity user-user similarity using cosine similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
